{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSeJSf5BzSe6/ZOSde4P+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BilawalBaloch/Tensorflow.NN/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5vGVz7ZqcNlG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 1. Prepare a simple, hardcoded dataset ---\n",
        "# A list of sentences\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You are a great person',\n",
        "    'I hate my dog',\n",
        "    'You are a terrible person',\n",
        "    'This is an amazing product',\n",
        "    'The food was delicious',\n",
        "    'The movie was terrible',\n",
        "    'I feel so sad today',\n",
        "    'It was a wonderful day',]"
      ],
      "metadata": {
        "id": "X4125UikcXvA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corresponding labels (0 for negative, 1 for positive)\n",
        "labels = [1, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "training_sentences = np.array(sentences)\n",
        "training_labels = np.array(labels)\n",
        "\n",
        "# --- 2. Tokenize the text data ---\n",
        "# This converts words into numerical representations\n",
        "# The num_words parameter limits the vocabulary size\n",
        "vocab_size = 100\n",
        "oov_token = \"<oov>\"  # Out-of-vocabulary token\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)"
      ],
      "metadata": {
        "id": "LLtsp8HlcdiF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the word index (a dictionary mapping words to integers)\n",
        "word_index = tokenizer.word_index\n",
        "print(f\"Word Index (first 10): {dict(list(word_index.items())[:10])}\\n\")\n",
        "\n",
        "# Convert sentences to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "\n",
        "# --- 3. Pad the sequences to ensure uniform length ---\n",
        "# Neural networks require input data of a consistent shape\n",
        "max_length = 8\n",
        "padding_type = 'post'  # 'post' means padding with zeros at the end\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type)\n",
        "\n",
        "print(f\"Padded Sequences:\\n{padded_sequences}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MGv8rvAcgVY",
        "outputId": "364200a1-d3ae-4754-b749-5fed9371bca9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index (first 10): {'<oov>': 1, 'i': 2, 'my': 3, 'a': 4, 'was': 5, 'love': 6, 'dog': 7, 'you': 8, 'are': 9, 'person': 10}\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 2  6  3  7  0  0  0  0]\n",
            " [ 2  6  3 13  0  0  0  0]\n",
            " [ 8  9  4 14 10  0  0  0]\n",
            " [ 2 15  3  7  0  0  0  0]\n",
            " [ 8  9  4 11 10  0  0  0]\n",
            " [16 17 18 19 20  0  0  0]\n",
            " [12 21  5 22  0  0  0  0]\n",
            " [12 23  5 11  0  0  0  0]\n",
            " [ 2 24 25 26 27  0  0  0]\n",
            " [28  5  4 29 30  0  0  0]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Build a simple neural network model ---\n",
        "embedding_dim = 16\n",
        "model = keras.Sequential([\n",
        "    # The Embedding layer learns a dense vector representation for each word\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # GlobalAveragePooling1D averages the vectors, reducing each sequence to a single vector\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    # The Dense layer is a standard fully connected layer\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    # The final Dense layer with a single neuron and sigmoid activation for binary classification\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUThph65cpO6",
        "outputId": "0796b286-51bd-428e-9489-fd3af7044ba8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary of the model's architecture\n",
        "model.summary()\n",
        "\n",
        "# --- 5. Compile and train the model ---\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "num_epochs = 100\n",
        "history = model.fit(padded_sequences, training_labels, epochs=num_epochs, verbose=2)\n",
        "\n",
        "# --- 6. Use the trained model to make predictions ---\n",
        "print(\"\\n--- Making Predictions ---\")\n",
        "test_sentences = [\n",
        "    \"I am very happy today\",\n",
        "    \"I'm feeling down\",\n",
        "    \"that was great\"\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R21I2e3Ccry1",
        "outputId": "13560733-9a39-4983-ce33-6ee7b6054643"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - 1s/step - accuracy: 0.6000 - loss: 0.6924\n",
            "Epoch 2/100\n",
            "1/1 - 0s - 46ms/step - accuracy: 0.6000 - loss: 0.6917\n",
            "Epoch 3/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6910\n",
            "Epoch 4/100\n",
            "1/1 - 0s - 74ms/step - accuracy: 0.6000 - loss: 0.6904\n",
            "Epoch 5/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6897\n",
            "Epoch 6/100\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.6000 - loss: 0.6891\n",
            "Epoch 7/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.6884\n",
            "Epoch 8/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6877\n",
            "Epoch 9/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6869\n",
            "Epoch 10/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6862\n",
            "Epoch 11/100\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.6000 - loss: 0.6855\n",
            "Epoch 12/100\n",
            "1/1 - 0s - 43ms/step - accuracy: 0.6000 - loss: 0.6849\n",
            "Epoch 13/100\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.6000 - loss: 0.6842\n",
            "Epoch 14/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6835\n",
            "Epoch 15/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6827\n",
            "Epoch 16/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6820\n",
            "Epoch 17/100\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.6000 - loss: 0.6812\n",
            "Epoch 18/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6804\n",
            "Epoch 19/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6796\n",
            "Epoch 20/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6788\n",
            "Epoch 21/100\n",
            "1/1 - 0s - 40ms/step - accuracy: 0.6000 - loss: 0.6779\n",
            "Epoch 22/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.6771\n",
            "Epoch 23/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.6761\n",
            "Epoch 24/100\n",
            "1/1 - 0s - 44ms/step - accuracy: 0.6000 - loss: 0.6752\n",
            "Epoch 25/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6743\n",
            "Epoch 26/100\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.6000 - loss: 0.6733\n",
            "Epoch 27/100\n",
            "1/1 - 0s - 43ms/step - accuracy: 0.6000 - loss: 0.6723\n",
            "Epoch 28/100\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.6000 - loss: 0.6714\n",
            "Epoch 29/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6704\n",
            "Epoch 30/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6693\n",
            "Epoch 31/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6683\n",
            "Epoch 32/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6673\n",
            "Epoch 33/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6662\n",
            "Epoch 34/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.6651\n",
            "Epoch 35/100\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.6000 - loss: 0.6640\n",
            "Epoch 36/100\n",
            "1/1 - 0s - 63ms/step - accuracy: 0.6000 - loss: 0.6629\n",
            "Epoch 37/100\n",
            "1/1 - 0s - 68ms/step - accuracy: 0.6000 - loss: 0.6617\n",
            "Epoch 38/100\n",
            "1/1 - 0s - 141ms/step - accuracy: 0.6000 - loss: 0.6605\n",
            "Epoch 39/100\n",
            "1/1 - 0s - 70ms/step - accuracy: 0.6000 - loss: 0.6593\n",
            "Epoch 40/100\n",
            "1/1 - 0s - 134ms/step - accuracy: 0.6000 - loss: 0.6580\n",
            "Epoch 41/100\n",
            "1/1 - 0s - 149ms/step - accuracy: 0.6000 - loss: 0.6567\n",
            "Epoch 42/100\n",
            "1/1 - 0s - 70ms/step - accuracy: 0.6000 - loss: 0.6554\n",
            "Epoch 43/100\n",
            "1/1 - 0s - 138ms/step - accuracy: 0.6000 - loss: 0.6541\n",
            "Epoch 44/100\n",
            "1/1 - 0s - 139ms/step - accuracy: 0.6000 - loss: 0.6528\n",
            "Epoch 45/100\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.6000 - loss: 0.6514\n",
            "Epoch 46/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.6500\n",
            "Epoch 47/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.6486\n",
            "Epoch 48/100\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.6000 - loss: 0.6472\n",
            "Epoch 49/100\n",
            "1/1 - 0s - 65ms/step - accuracy: 0.6000 - loss: 0.6457\n",
            "Epoch 50/100\n",
            "1/1 - 0s - 141ms/step - accuracy: 0.6000 - loss: 0.6443\n",
            "Epoch 51/100\n",
            "1/1 - 0s - 138ms/step - accuracy: 0.6000 - loss: 0.6428\n",
            "Epoch 52/100\n",
            "1/1 - 0s - 144ms/step - accuracy: 0.6000 - loss: 0.6413\n",
            "Epoch 53/100\n",
            "1/1 - 0s - 67ms/step - accuracy: 0.6000 - loss: 0.6397\n",
            "Epoch 54/100\n",
            "1/1 - 0s - 135ms/step - accuracy: 0.6000 - loss: 0.6382\n",
            "Epoch 55/100\n",
            "1/1 - 0s - 73ms/step - accuracy: 0.6000 - loss: 0.6366\n",
            "Epoch 56/100\n",
            "1/1 - 0s - 63ms/step - accuracy: 0.6000 - loss: 0.6350\n",
            "Epoch 57/100\n",
            "1/1 - 0s - 49ms/step - accuracy: 0.6000 - loss: 0.6333\n",
            "Epoch 58/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6317\n",
            "Epoch 59/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6300\n",
            "Epoch 60/100\n",
            "1/1 - 0s - 47ms/step - accuracy: 0.6000 - loss: 0.6283\n",
            "Epoch 61/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6266\n",
            "Epoch 62/100\n",
            "1/1 - 0s - 70ms/step - accuracy: 0.6000 - loss: 0.6248\n",
            "Epoch 63/100\n",
            "1/1 - 0s - 50ms/step - accuracy: 0.6000 - loss: 0.6230\n",
            "Epoch 64/100\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.6000 - loss: 0.6212\n",
            "Epoch 65/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6193\n",
            "Epoch 66/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6174\n",
            "Epoch 67/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.6155\n",
            "Epoch 68/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6135\n",
            "Epoch 69/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.6115\n",
            "Epoch 70/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6095\n",
            "Epoch 71/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6075\n",
            "Epoch 72/100\n",
            "1/1 - 0s - 40ms/step - accuracy: 0.6000 - loss: 0.6055\n",
            "Epoch 73/100\n",
            "1/1 - 0s - 44ms/step - accuracy: 0.6000 - loss: 0.6034\n",
            "Epoch 74/100\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.6000 - loss: 0.6013\n",
            "Epoch 75/100\n",
            "1/1 - 0s - 40ms/step - accuracy: 0.6000 - loss: 0.5992\n",
            "Epoch 76/100\n",
            "1/1 - 0s - 47ms/step - accuracy: 0.6000 - loss: 0.5971\n",
            "Epoch 77/100\n",
            "1/1 - 0s - 43ms/step - accuracy: 0.6000 - loss: 0.5949\n",
            "Epoch 78/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.6000 - loss: 0.5927\n",
            "Epoch 79/100\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.6000 - loss: 0.5905\n",
            "Epoch 80/100\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.6000 - loss: 0.5883\n",
            "Epoch 81/100\n",
            "1/1 - 0s - 46ms/step - accuracy: 0.6000 - loss: 0.5860\n",
            "Epoch 82/100\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.6000 - loss: 0.5837\n",
            "Epoch 83/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.5814\n",
            "Epoch 84/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.6000 - loss: 0.5791\n",
            "Epoch 85/100\n",
            "1/1 - 0s - 63ms/step - accuracy: 0.6000 - loss: 0.5767\n",
            "Epoch 86/100\n",
            "1/1 - 0s - 133ms/step - accuracy: 0.7000 - loss: 0.5744\n",
            "Epoch 87/100\n",
            "1/1 - 0s - 40ms/step - accuracy: 0.7000 - loss: 0.5720\n",
            "Epoch 88/100\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.7000 - loss: 0.5696\n",
            "Epoch 89/100\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.7000 - loss: 0.5672\n",
            "Epoch 90/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.7000 - loss: 0.5648\n",
            "Epoch 91/100\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.7000 - loss: 0.5623\n",
            "Epoch 92/100\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.7000 - loss: 0.5598\n",
            "Epoch 93/100\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.7000 - loss: 0.5573\n",
            "Epoch 94/100\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.7000 - loss: 0.5548\n",
            "Epoch 95/100\n",
            "1/1 - 0s - 42ms/step - accuracy: 0.7000 - loss: 0.5522\n",
            "Epoch 96/100\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.7000 - loss: 0.5496\n",
            "Epoch 97/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.7000 - loss: 0.5470\n",
            "Epoch 98/100\n",
            "1/1 - 0s - 44ms/step - accuracy: 0.7000 - loss: 0.5443\n",
            "Epoch 99/100\n",
            "1/1 - 0s - 48ms/step - accuracy: 0.7000 - loss: 0.5416\n",
            "Epoch 100/100\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.7000 - loss: 0.5389\n",
            "\n",
            "--- Making Predictions ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the new sentences just like the training data\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type)\n",
        "\n",
        "# Predict the sentiment (0-1, where closer to 1 is positive)\n",
        "predictions = model.predict(test_padded_sequences)\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    sentiment = \"Positive\" if predictions[i][0] > 0.5 else \"Negative\"\n",
        "    print(f\"'{sentence}' -> Prediction: {predictions[i][0]:.4f}, Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jSK5JnucvFt",
        "outputId": "d0d62e4a-0861-4f23-aa81-48463f99138c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "'I am very happy today' -> Prediction: 0.5680, Sentiment: Positive\n",
            "'I'm feeling down' -> Prediction: 0.6485, Sentiment: Positive\n",
            "'that was great' -> Prediction: 0.6929, Sentiment: Positive\n"
          ]
        }
      ]
    }
  ]
}